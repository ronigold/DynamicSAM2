{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d66a42d1-89ed-4f15-bb95-ac858be73b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "\n",
    "from dynamic_sam2.sam2_video_tracker import Sam2VideoTracker\n",
    "from dynamic_sam2.object_detection import DinoDetectionModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d521d48-4ad0-47c5-bd62-0046a79a38fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3595.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final text_encoder_type: bert-base-uncased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint path: /home/ubuntu/DynamicSAM2/checkpoints/sam2.1_hiera_large.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:Sam2VideoTracker:\n",
      "=== Starting Video Processing ===\n",
      "DEBUG:Sam2VideoTracker:Prepared 25 frames for chunk 0-24\n",
      "INFO:Sam2VideoTracker:Processing chunk: current_frame = 0, chunk_end = 24, frames in chunk = 25\n",
      "UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "DEBUG:Sam2VideoTracker:Creating masks for 6 boxes\n",
      "UserWarning: Memory efficient kernel not used because: (Triggered internally at ../aten/src/ATen/native/transformers/cuda/sdp_utils.cpp:773.)\n",
      "UserWarning: Memory Efficient attention has been runtime disabled. (Triggered internally at ../aten/src/ATen/native/transformers/sdp_utils_cpp.h:558.)\n",
      "UserWarning: Flash attention kernel not used because: (Triggered internally at ../aten/src/ATen/native/transformers/cuda/sdp_utils.cpp:775.)\n",
      "UserWarning: Expected query, key and value to all be of dtype: {Half, BFloat16}. Got Query dtype: float, Key dtype: float, and Value dtype: float instead. (Triggered internally at ../aten/src/ATen/native/transformers/sdp_utils_cpp.h:100.)\n",
      "UserWarning: CuDNN attention kernel not used because: (Triggered internally at ../aten/src/ATen/native/transformers/cuda/sdp_utils.cpp:777.)\n",
      "UserWarning: Flash Attention kernel failed due to: No available kernel. Aborting execution.\n",
      "Falling back to all available kernels for scaled_dot_product_attention (which may have a slower speed).\n",
      "frame loading (JPEG): 100%|██████████| 25/25 [00:00<00:00, 35.05it/s]\n",
      "propagate in video: 100%|██████████| 25/25 [00:14<00:00,  1.69it/s]\n",
      "INFO:Sam2VideoTracker:\n",
      "=== Starting Detection Merge ===\n",
      "INFO:Sam2VideoTracker:Active SAM2 tracks after filtering: 5\n",
      "INFO:Sam2VideoTracker:Filtered SAM2 tracks remaining: 5\n",
      "INFO:Sam2VideoTracker:Active Track 1: ID 1, Box [  0 392 156 711]\n",
      "INFO:Sam2VideoTracker:Active Track 2: ID 2, Box [268  24 576 660]\n",
      "INFO:Sam2VideoTracker:Active Track 3: ID 3, Box [704 438 902 605]\n",
      "INFO:Sam2VideoTracker:Active Track 4: ID 4, Box [566 476 714 589]\n",
      "INFO:Sam2VideoTracker:Active Track 5: ID 5, Box [  0 544 929 719]\n",
      "INFO:Sam2VideoTracker:DINO new detections: 5\n",
      "INFO:Sam2VideoTracker:New Detection 1: Label person <, Box [262.7683   21.15271 582.22144 668.1216 ]\n",
      "INFO:Sam2VideoTracker:New Detection 2: Label > bed, Box [  3.6622314 515.96185   965.54285   717.96735  ]\n",
      "INFO:Sam2VideoTracker:New Detection 3: Label > pillow <, Box [701.6552  433.59198 911.1701  611.0667 ]\n",
      "INFO:Sam2VideoTracker:New Detection 4: Label > bed, Box [  3.807495 440.16434  966.2313   718.1848  ]\n",
      "INFO:Sam2VideoTracker:New Detection 5: Label pillow <, Box [560.3252 467.8506 719.0906 597.0769]\n",
      "INFO:Sam2VideoTracker:=== Merge Summary ===\n",
      "INFO:Sam2VideoTracker:Initial active tracked objects: 5\n",
      "INFO:Sam2VideoTracker:New objects added: 0\n",
      "INFO:Sam2VideoTracker:Final object count: 5\n",
      "INFO:Sam2VideoTracker:Final Object 1: ID 1, Box [  0 392 156 711]\n",
      "INFO:Sam2VideoTracker:Final Object 2: ID 2, Box [268  24 576 660]\n",
      "INFO:Sam2VideoTracker:Final Object 3: ID 3, Box [704 438 902 605]\n",
      "INFO:Sam2VideoTracker:Final Object 4: ID 4, Box [566 476 714 589]\n",
      "INFO:Sam2VideoTracker:Final Object 5: ID 5, Box [  0 544 929 719]\n",
      "DEBUG:Sam2VideoTracker:Cleaned up chunk directory: temp_frames/chunk_0_24\n",
      "DEBUG:Sam2VideoTracker:Prepared 10 frames for chunk 24-33\n",
      "INFO:Sam2VideoTracker:Processing chunk: current_frame = 24, chunk_end = 33, frames in chunk = 10\n",
      "frame loading (JPEG): 100%|██████████| 10/10 [00:00<00:00, 34.43it/s]\n",
      "propagate in video: 100%|██████████| 10/10 [00:04<00:00,  2.02it/s]\n",
      "INFO:Sam2VideoTracker:\n",
      "=== Starting Detection Merge ===\n",
      "INFO:Sam2VideoTracker:Active SAM2 tracks after filtering: 5\n",
      "INFO:Sam2VideoTracker:Filtered SAM2 tracks remaining: 5\n",
      "INFO:Sam2VideoTracker:Active Track 1: ID 1, Box [132   0 349 567]\n",
      "INFO:Sam2VideoTracker:Active Track 2: ID 2, Box [391  62 632 481]\n",
      "INFO:Sam2VideoTracker:Active Track 3: ID 3, Box [742 455 939 624]\n",
      "INFO:Sam2VideoTracker:Active Track 4: ID 4, Box [603 494 750 608]\n",
      "INFO:Sam2VideoTracker:Active Track 5: ID 5, Box [  0 559 963 719]\n",
      "INFO:Sam2VideoTracker:DINO new detections: 6\n",
      "INFO:Sam2VideoTracker:New Detection 1: Label person <, Box [385.79526  58.94545 639.34204 487.09253]\n",
      "INFO:Sam2VideoTracker:New Detection 2: Label > bed, Box [   3.4224854  457.69043   1004.64844    718.0847   ]\n",
      "INFO:Sam2VideoTracker:New Detection 3: Label person <, Box [126.29602    2.643402 355.995    574.24084 ]\n",
      "INFO:Sam2VideoTracker:New Detection 4: Label > bed, Box [   2.9276733  541.06      1004.17914    717.75323  ]\n",
      "INFO:Sam2VideoTracker:New Detection 5: Label > pillow <, Box [737.9439 450.9115 948.6192 629.5967]\n",
      "INFO:Sam2VideoTracker:New Detection 6: Label > pillow <, Box [377.08954 495.7537  583.2646  598.6617 ]\n",
      "DEBUG:Sam2VideoTracker:Detection 6 has no overlap - adding as new object\n",
      "DEBUG:Sam2VideoTracker:Creating masks for 1 boxes\n",
      "INFO:Sam2VideoTracker:Added new object with ID 6, Box [377.08954 495.7537  583.2646  598.6617 ]\n",
      "INFO:Sam2VideoTracker:=== Merge Summary ===\n",
      "INFO:Sam2VideoTracker:Initial active tracked objects: 5\n",
      "INFO:Sam2VideoTracker:New objects added: 1\n",
      "INFO:Sam2VideoTracker:Final object count: 6\n",
      "INFO:Sam2VideoTracker:Final Object 1: ID 1, Box [132.   0. 349. 567.]\n",
      "INFO:Sam2VideoTracker:Final Object 2: ID 2, Box [391.  62. 632. 481.]\n",
      "INFO:Sam2VideoTracker:Final Object 3: ID 3, Box [742. 455. 939. 624.]\n",
      "INFO:Sam2VideoTracker:Final Object 4: ID 4, Box [603. 494. 750. 608.]\n",
      "INFO:Sam2VideoTracker:Final Object 5: ID 5, Box [  0. 559. 963. 719.]\n",
      "INFO:Sam2VideoTracker:Final Object 6: ID 6, Box [377.08953857 495.75369263 583.2645874  598.66168213]\n",
      "DEBUG:Sam2VideoTracker:Cleaned up chunk directory: temp_frames/chunk_24_33\n",
      "INFO:Sam2VideoTracker:Filtered out 1 objects with fewer than 5 valid frames\n",
      "INFO:Sam2VideoTracker:Creating final video with only valid objects...\n",
      "DEBUG:Sam2VideoTracker:First frame path: tracking_results/00000_tracked.jpg\n",
      "DEBUG:Sam2VideoTracker:First frame exists: True\n",
      "INFO:Sam2VideoTracker:Creating video from 34 frames\n",
      "INFO:Sam2VideoTracker:Target FPS: 5\n",
      "INFO:Sam2VideoTracker:Output path: tracking_results/tracked.mp4\n",
      "INFO:Sam2VideoTracker:Creating video with OpenCV\n",
      "INFO:Sam2VideoTracker:Creating video at 5 fps, resolution: 1280x720\n",
      "INFO:Sam2VideoTracker:Successfully opened writer with codec mp4v\n",
      "Writing frames: 100%|██████████| 34/34 [00:00<00:00, 97.53it/s]\n",
      "INFO:Sam2VideoTracker:Successfully wrote 34/34 frames\n",
      "INFO:Sam2VideoTracker:OpenCV video saved at: tracking_results/tracked.mp4\n",
      "INFO:Sam2VideoTracker:Renamed output video to: tracking_results/bedroom_tracked.mp4\n",
      "INFO:Sam2VideoTracker:Tracking complete. Found 5 unique objects after filtering.\n",
      "DEBUG:Sam2VideoTracker:Cleaned up temporary frames directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 36.5 s, sys: 3.29 s, total: 39.8 s\n",
      "Wall time: 31 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "\n",
    "dino_model = DinoDetectionModel(\n",
    "    device=\"cuda\",\n",
    "    box_threshold=0.30,\n",
    "    text_threshold=0.25\n",
    ")\n",
    "\n",
    "tracker = Sam2VideoTracker(\n",
    "    video_path=\"../assets/bedroom.mp4\",\n",
    "    text_prompt=\"person <and> pillow <and> bed\",\n",
    "    detection_model=dino_model,\n",
    "    output_dir=\"tracking_results\",\n",
    "    frames_dir=\"temp_frames\",\n",
    "    check_interval=25,\n",
    "    device=\"cuda\",\n",
    "    target_fps=5,\n",
    "    target_resolution=(1280, 720),\n",
    "    save_masks=True\n",
    ")\n",
    "\n",
    "obj = tracker.process_video()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be07b953-37fa-407f-a747-bb54056a840e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([1, 2, 3, 4, 5])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c07fed1-0893-4545-902c-a7deeca1d7fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [195, 180, 389, 539],\n",
       " 1: [231, 179, 402, 504],\n",
       " 2: [251, 325, 414, 589],\n",
       " 3: [237, 224, 392, 560],\n",
       " 4: [233, 156, 392, 512],\n",
       " 5: [261, 295, 400, 583],\n",
       " 6: [253, 256, 417, 579],\n",
       " 7: [286, 189, 424, 508],\n",
       " 8: [335, 366, 483, 558],\n",
       " 9: [353, 325, 448, 560],\n",
       " 10: [338, 235, 507, 497],\n",
       " 11: [297, 365, 522, 548],\n",
       " 12: [335, 415, 493, 570],\n",
       " 13: [343, 292, 459, 510],\n",
       " 14: [222, 309, 435, 500],\n",
       " 15: [252, 427, 418, 573],\n",
       " 16: [173, 209, 428, 553],\n",
       " 17: [120, 212, 402, 502],\n",
       " 18: [137, 367, 341, 632],\n",
       " 19: [91, 253, 318, 589],\n",
       " 20: [0, 219, 230, 550],\n",
       " 21: [0, 358, 221, 667],\n",
       " 22: [0, 237, 262, 608],\n",
       " 23: [0, 218, 173, 610],\n",
       " 24: [0, 392, 156, 711],\n",
       " 25: [0, 314, 164, 690],\n",
       " 26: [0, 67, 216, 656],\n",
       " 27: [22, 0, 217, 600],\n",
       " 28: [0, 114, 234, 705],\n",
       " 29: [55, 0, 255, 650],\n",
       " 30: [65, 0, 282, 565],\n",
       " 31: [0, 62, 303, 682],\n",
       " 32: [76, 74, 272, 697],\n",
       " 33: [132.0, 0.0, 349.0, 567.0]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj[1]['frames']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7914c15-1db4-4be1-b41d-437e881654eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'person <'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj[1]['class']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Grounded-Sam2",
   "language": "python",
   "name": "grounded_sam2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
